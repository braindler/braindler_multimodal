#!/usr/bin/env python3
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –Ω–æ—á–Ω–æ–≥–æ —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç:
1. –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã (897 –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ 30 —Ç–æ–º–æ–≤)
2. Alpaca dataset (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞)
3. Kene multimodal (–¥—É—Ö–æ–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã)

¬© 2025 NativeMind - NativeMindNONC License
"""

import json
import os
from datasets import load_dataset, concatenate_datasets, Dataset

def load_legal_dataset(path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç"""
    print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset = load_dataset('json', data_files=path, split='train')
    print(f"   ‚úÖ –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
    return dataset

def load_alpaca_dataset(path, max_examples=5000):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç Alpaca –¥–∞—Ç–∞—Å–µ—Ç"""
    print("\nü¶ô –ó–∞–≥—Ä—É–∑–∫–∞ Alpaca –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset = load_dataset('json', data_files=path, split='train')
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ)
    keywords = ['analyze', 'evaluate', 'review', 'compare', 'assess', 
                'examine', 'investigate', 'judge', 'legal', 'evidence']
    
    filtered = dataset.filter(
        lambda x: any(word in x['instruction'].lower() for word in keywords)
    )
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    if len(filtered) > max_examples:
        filtered = filtered.select(range(max_examples))
    
    print(f"   ‚úÖ Alpaca –ø—Ä–∏–º–µ—Ä–æ–≤ (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ): {len(filtered)}")
    
    # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç
    def adapt_format(example):
        return {
            "instruction": example['instruction'],
            "input": example.get('input', ''),
            "output": example['output'],
            "sphere": "general",  # –û–±—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã
            "source": "alpaca"
        }
    
    adapted = filtered.map(adapt_format)
    return adapted

def load_kene_spiritual_texts(path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥—É—Ö–æ–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ Kene dataset"""
    print("\nüïâÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –¥—É—Ö–æ–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ Kene...")
    
    spiritual_texts = []
    
    # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç—ã –®—Ä–∏–¥—Ö–∞—Ä–∞ –ú–∞—Ö–∞—Ä–∞–¥–∂–∞
    spiritual_dir = os.path.join(path, "–ö–Ω–∏–≥–∏/–î—É—Ö–æ–≤–Ω–æ–µ/–í–∞–π—à–Ω–∞–≤–∏–∑–º/–®—Ä–∏–¥—Ö–∞—Ä –ú–∞—Ö–∞—Ä–∞–¥–∂")
    
    if os.path.exists(spiritual_dir):
        for file in os.listdir(spiritual_dir):
            if file.endswith('.txt'):
                try:
                    with open(os.path.join(spiritual_dir, file), 'r', encoding='utf-8') as f:
                        text = f.read()
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                        paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 100]
                        
                        for para in paragraphs[:10]:  # –ü–µ—Ä–≤—ã–µ 10 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                            spiritual_texts.append({
                                "instruction": "–û–±—ä—è—Å–Ω–∏ –¥—É—Ö–æ–≤–Ω—ã–π —Å–º—ã—Å–ª —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
                                "input": para[:800],
                                "output": "–° –¥—É—Ö–æ–≤–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è, —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç —É—á–∏—Ç –Ω–∞—Å —Å–ª—É–∂–µ–Ω–∏—é –∏—Å—Ç–∏–Ω–µ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏. –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –≥–ª—É–±–∏–Ω–Ω—ã–π —Å–º—ã—Å–ª, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É.",
                                "sphere": "001",  # –î—É—Ö–æ–≤–Ω–∞—è —Å—Ñ–µ—Ä–∞
                                "source": "kene_spiritual"
                            })
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  {file}: {e}")
    
    print(f"   ‚úÖ –î—É—Ö–æ–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(spiritual_texts)}")
    
    return Dataset.from_list(spiritual_texts) if spiritual_texts else None

def prepare_combined_dataset(
    legal_path="../datasets/legal_case_viktor/legal_dataset.jsonl",
    alpaca_path="/Volumes/MULTIMODAL/proj.datasets/alpaca_dataset/alpaca_data.json",
    kene_path="/Volumes/MULTIMODAL/proj.datasets/kene_multimodal_gift",
    output_path="../datasets/combined_legal_training.jsonl"
):
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    """
    print("="*80)
    print("üî• –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è M4")
    print("="*80)
    print()
    
    datasets_to_combine = []
    
    # 1. –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    try:
        legal = load_legal_dataset(legal_path)
        datasets_to_combine.append(legal)
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    
    # 2. Alpaca –¥–∞—Ç–∞—Å–µ—Ç
    try:
        alpaca = load_alpaca_dataset(alpaca_path, max_examples=3000)
        datasets_to_combine.append(alpaca)
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ Alpaca –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    
    # 3. –î—É—Ö–æ–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã Kene
    try:
        kene = load_kene_spiritual_texts(kene_path)
        if kene:
            datasets_to_combine.append(kene)
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ Kene –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
    print(f"\nüîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    combined = concatenate_datasets(datasets_to_combine)
    
    print(f"   ‚úÖ –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(combined)}")
    print()
    print("   üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ñ–µ—Ä–∞–º
    sphere_counts = {}
    for item in combined:
        sphere = item.get('sphere', 'unknown')
        sphere_counts[sphere] = sphere_counts.get(sphere, 0) + 1
    
    for sphere, count in sorted(sphere_counts.items()):
        sphere_name = {
            "001": "–î—É—Ö–æ–≤–Ω–∞—è",
            "047": "–°–õ–ï–î–û–í–ê–¢–ï–õ–¨",
            "048": "–ü–†–û–ö–£–†–û–†",
            "049": "–°–£–î–¨–Ø",
            "general": "Alpaca –æ–±—â–∏–µ"
        }.get(sphere, sphere)
        print(f"      –°—Ñ–µ—Ä–∞ {sphere} ({sphere_name}): {count}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # JSONL —Ñ–æ—Ä–º–∞—Ç
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in combined:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        "total_examples": len(combined),
        "sphere_distribution": sphere_counts,
        "sources": list(set([item.get('source', 'unknown') for item in combined]))
    }
    
    stats_path = output_path.replace('.jsonl', '_stats.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats_path}")
    
    print("\n‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤!")
    print(f"üôè –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(combined)}")
    
    return combined

if __name__ == "__main__":
    prepare_combined_dataset()





