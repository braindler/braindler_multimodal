#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ —Å—Ñ–µ—Ä —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (073-078) —Å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º

–£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞

¬© 2025 NativeMind - NativeMindNONC License
"""

import os
import sys
import torch
import argparse
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
import json

def finetune_developer_sphere_improved(
    sphere,
    dataset_path,
    output_dir,
    base_model="nativemind/shridhar_8k_multimodal",
    epochs=5,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 5
    batch_size=4,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 2 –¥–æ 4
    learning_rate=1e-4,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 2e-4 –¥–æ 1e-4
    use_8bit=False
):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ñ–µ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    
    Args:
        sphere: –ù–æ–º–µ—Ä —Å—Ñ–µ—Ä—ã ("073", "074", "075", "076", "077", "078")
        dataset_path: –ü—É—Ç—å –∫ JSONL –¥–∞—Ç–∞—Å–µ—Ç—É
        output_dir: –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        base_model: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 5)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 4)
        learning_rate: Learning rate (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–æ 1e-4)
        use_8bit: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 8-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
    """
    
    sphere_names = {
        "073": "DEVELOPER",
        "074": "CODE_REVIEWER", 
        "075": "ARCHITECT",
        "076": "DEVOPS",
        "077": "QA_TESTER",
        "078": "TECH_WRITER"
    }
    
    print("="*80)
    print(f"üöÄ –£–õ–£–ß–®–ï–ù–ù–´–ô –§–ê–ô–ù–¢–Æ–ù–ò–ù–ì –°—Ñ–µ—Ä—ã {sphere}: {sphere_names[sphere]}")
    print("="*80)
    print()
    print(f"üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model}")
    print(f"üìö –î–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
    print(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    print(f"üéØ –≠–ø–æ—Ö: {epochs}, Batch size: {batch_size}, LR: {learning_rate}")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model_kwargs = {
        "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
        "device_map": "auto" if device == "cuda" else None,
    }
    
    if use_8bit and device == "cuda":
        model_kwargs["load_in_8bit"] = True
        model_kwargs["device_map"] = "auto"
    
    model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)
    
    if device == "mps":
        model = model.to("mps")
    elif device == "cpu":
        model = model.to("cpu")
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device.upper()}")
    print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.num_parameters():,}")
    
    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
    print("\nüîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA (—É–ª—É—á—à–µ–Ω–Ω–∞—è)...")
    lora_config = LoraConfig(
        r=32,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 16 –¥–æ 32 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        lora_alpha=64,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 32 –¥–æ 64
        lora_dropout=0.05,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 0.1 –¥–æ 0.05
        target_modules=["c_attn", "c_proj", "c_fc"],  # –î–æ–±–∞–≤–ª–µ–Ω c_fc
        task_type=TaskType.CAUSAL_LM,
    )
    
    if use_8bit and device == "cuda":
        model = prepare_model_for_kbit_training(model)
    
    model = get_peft_model(model, lora_config)
    print("   ‚úÖ LoRA –ø—Ä–∏–º–µ–Ω–µ–Ω (—É–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)")
    print(f"   üéØ –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset = load_dataset('json', data_files=dataset_path, split='train')
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å—Ñ–µ—Ä–µ
    def filter_sphere(example):
        return example.get('sphere') == sphere
    
    filtered_dataset = dataset.filter(filter_sphere)
    print(f"   üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –°—Ñ–µ—Ä—ã {sphere}: {len(filtered_dataset)}")
    
    # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    def tokenize_function(examples):
        texts = []
        for i in range(len(examples['instruction'])):
            instruction = examples['instruction'][i]
            input_text = examples['input'][i] if examples['input'][i] else ""
            output = examples['output'][i]
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞
            if input_text:
                text = f"### –†–æ–ª—å: {sphere_names[sphere]}\n### –ó–∞–¥–∞—á–∞:\n{instruction}\n\n### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n{input_text}\n\n### –†–µ—à–µ–Ω–∏–µ:\n{output}"
            else:
                text = f"### –†–æ–ª—å: {sphere_names[sphere]}\n### –ó–∞–¥–∞—á–∞:\n{instruction}\n\n### –†–µ—à–µ–Ω–∏–µ:\n{output}"
            
            texts.append(text)
        
        return tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=4096,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 2048 –¥–æ 4096
            return_tensors="pt"
        )
    
    print("   üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—É–ª—É—á—à–µ–Ω–Ω–∞—è)...")
    tokenized_dataset = filtered_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=filtered_dataset.column_names
    )
    print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(tokenized_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=8,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 4 –¥–æ 8
        learning_rate=learning_rate,
        warmup_steps=200,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100 –¥–æ 200
        logging_steps=5,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 10 –¥–æ 5 –¥–ª—è –±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        save_steps=50,  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 100 –¥–æ 50 –¥–ª—è –±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_strategy="steps",
        load_best_model_at_end=False,
        report_to=None,
        remove_unused_columns=False,
        dataloader_pin_memory=False if device == "mps" else True,
        # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        fp16=torch.cuda.is_available(),  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fp16 –Ω–∞ GPU
        dataloader_num_workers=0,  # –û—Ç–∫–ª—é—á–∏—Ç—å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        seed=42,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        weight_decay=0.01,  # –î–æ–±–∞–≤–∏—Ç—å weight decay
        adam_beta1=0.9,
        adam_beta2=0.999,
        adam_epsilon=1e-8,
    )
    
    print(f"\n‚öôÔ∏è  –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"   ‚úÖ –≠–ø–æ—Ö: {epochs} (—É–≤–µ–ª–∏—á–µ–Ω–æ)")
    print(f"   ‚úÖ Batch size: {batch_size} (—É–≤–µ–ª–∏—á–µ–Ω–æ)")
    print(f"   ‚úÖ –ì—Ä–∞–¥–∏–µ–Ω—Ç accumulation: 8 (—É–≤–µ–ª–∏—á–µ–Ω–æ)")
    print(f"   ‚úÖ Learning rate: {learning_rate} (—É–º–µ–Ω—å—à–µ–Ω–æ)")
    print(f"   ‚úÖ Warmup steps: 200 (—É–≤–µ–ª–∏—á–µ–Ω–æ)")
    print(f"   ‚úÖ Max length: 4096 (—É–≤–µ–ª–∏—á–µ–Ω–æ)")
    print(f"   ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch: {batch_size * 8}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Trainer
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    print("\nüèóÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ Trainer...")
    print("   ‚úÖ Trainer –≥–æ—Ç–æ–≤ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "="*80)
    print(f"üèãÔ∏è  –ù–ê–ß–ò–ù–ê–ï–ú –£–õ–£–ß–®–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –°—Ñ–µ—Ä—ã {sphere}: {sphere_names[sphere]}")
    print("="*80)
    print()
    print("üôè –î—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è: –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ AI")
    print(f"‚è±Ô∏è  –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{epochs * len(tokenized_dataset) // (batch_size * 8) // 60} –º–∏–Ω—É—Ç")
    print()
    
    try:
        trainer.train()
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")
        
        print("\n" + "="*80)
        print(f"‚úÖ –°–§–ï–†–ê {sphere} –£–õ–£–ß–®–ï–ù–ê!")
        print("="*80)
        print(f"üìÅ {output_dir}")
        print("üôè –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ —Å—Ñ–µ—Ä —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏")
    parser.add_argument("--sphere", required=True, choices=["073", "074", "075", "076", "077", "078"],
                       help="–ù–æ–º–µ—Ä —Å—Ñ–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--dataset", default="../datasets/developers_high_quality.jsonl",
                       help="–ü—É—Ç—å –∫ —É–ª—É—á—à–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument("--output", default="../models",
                       help="–ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    parser.add_argument("--base-model", default="nativemind/shridhar_8k_multimodal",
                       help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
    parser.add_argument("--epochs", type=int, default=5, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (—É–ª—É—á—à–µ–Ω–æ)")
    parser.add_argument("--batch-size", type=int, default=4, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—É–ª—É—á—à–µ–Ω–æ)")
    parser.add_argument("--learning-rate", type=float, default=1e-4, help="Learning rate (—É–ª—É—á—à–µ–Ω–æ)")
    parser.add_argument("--use-8bit", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 8-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ")
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    output_dir = f"{args.output}/sphere_{args.sphere}_improved"
    os.makedirs(output_dir, exist_ok=True)
    
    # –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    success = finetune_developer_sphere_improved(
        sphere=args.sphere,
        dataset_path=args.dataset,
        output_dir=output_dir,
        base_model=args.base_model,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        use_8bit=args.use_8bit
    )
    
    if success:
        print(f"\nüéâ –°—Ñ–µ—Ä–∞ {args.sphere} —É—Å–ø–µ—à–Ω–æ —É–ª—É—á—à–µ–Ω–∞!")
        sys.exit(0)
    else:
        print(f"\nüí• –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ —Å—Ñ–µ—Ä—ã {args.sphere}")
        sys.exit(1)

if __name__ == "__main__":
    main()
