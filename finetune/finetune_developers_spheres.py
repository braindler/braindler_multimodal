#!/usr/bin/env python3
"""
–§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ —Å—Ñ–µ—Ä —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (073-078) —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é shridhar_8k_multimodal

–°—Ñ–µ—Ä—ã:
- 073: DEVELOPER
- 074: CODE_REVIEWER  
- 075: ARCHITECT
- 076: DEVOPS
- 077: QA_TESTER
- 078: TECH_WRITER

¬© 2025 NativeMind - NativeMindNONC License
"""

import os
import sys
import torch
import argparse
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
import json

def finetune_developer_sphere(
    sphere,
    dataset_path,
    output_dir,
    base_model="nativemind/shridhar_8k_multimodal",
    epochs=3,
    batch_size=2,
    learning_rate=2e-4,
    use_8bit=False
):
    """
    –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ñ–µ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    
    Args:
        sphere: –ù–æ–º–µ—Ä —Å—Ñ–µ—Ä—ã ("073", "074", "075", "076", "077", "078")
        dataset_path: –ü—É—Ç—å –∫ JSONL –¥–∞—Ç–∞—Å–µ—Ç—É
        output_dir: –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        base_model: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate: Learning rate
        use_8bit: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 8-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
    """
    
    sphere_names = {
        "073": "DEVELOPER",
        "074": "CODE_REVIEWER", 
        "075": "ARCHITECT",
        "076": "DEVOPS",
        "077": "QA_TESTER",
        "078": "TECH_WRITER"
    }
    
    print("="*80)
    print(f"üíª –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –°—Ñ–µ—Ä—ã {sphere}: {sphere_names[sphere]}")
    print("="*80)
    print()
    print(f"üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model}")
    print(f"üìö –î–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
    print(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
    print(f"üéØ –≠–ø–æ—Ö: {epochs}, Batch size: {batch_size}, LR: {learning_rate}")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model_kwargs = {
        "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
        "device_map": "auto" if device == "cuda" else None,
    }
    
    if use_8bit and device == "cuda":
        model_kwargs["load_in_8bit"] = True
        model_kwargs["device_map"] = "auto"
    
    model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)
    
    if device == "mps":
        model = model.to("mps")
    elif device == "cpu":
        model = model.to("cpu")
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device.upper()}")
    print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.num_parameters():,}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
    print("\nüîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...")
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["c_attn", "c_proj"],
        task_type=TaskType.CAUSAL_LM,
    )
    
    if use_8bit and device == "cuda":
        model = prepare_model_for_kbit_training(model)
    
    model = get_peft_model(model, lora_config)
    print("   ‚úÖ LoRA –ø—Ä–∏–º–µ–Ω–µ–Ω")
    print(f"   üéØ –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset = load_dataset('json', data_files=dataset_path, split='train')
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å—Ñ–µ—Ä–µ
    def filter_sphere(example):
        return example.get('sphere') == sphere
    
    filtered_dataset = dataset.filter(filter_sphere)
    print(f"   üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –°—Ñ–µ—Ä—ã {sphere}: {len(filtered_dataset)}")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    def tokenize_function(examples):
        texts = []
        for i in range(len(examples['instruction'])):
            instruction = examples['instruction'][i]
            input_text = examples['input'][i] if examples['input'][i] else ""
            output = examples['output'][i]
            
            if input_text:
                text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
            else:
                text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
            
            texts.append(text)
        
        return tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=2048,
            return_tensors="pt"
        )
    
    print("   üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    tokenized_dataset = filtered_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=filtered_dataset.column_names
    )
    print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(tokenized_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=4,
        learning_rate=learning_rate,
        warmup_steps=100,
        logging_steps=10,
        save_steps=100,
        save_strategy="steps",
        load_best_model_at_end=False,
        report_to=None,
        remove_unused_columns=False,
        dataloader_pin_memory=False if device == "mps" else True,
    )
    
    print(f"\n‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
    print(f"   ‚úÖ –≠–ø–æ—Ö: {epochs}")
    print(f"   ‚úÖ Batch size: {batch_size}")
    print(f"   ‚úÖ –ì—Ä–∞–¥–∏–µ–Ω—Ç accumulation: 4")
    print(f"   ‚úÖ Learning rate: {learning_rate}")
    print(f"   ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch: {batch_size * 4}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Trainer
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    print("\nüèóÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ Trainer...")
    print("   ‚úÖ Trainer –≥–æ—Ç–æ–≤")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("\n" + "="*80)
    print(f"üèãÔ∏è  –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –°—Ñ–µ—Ä—ã {sphere}: {sphere_names[sphere]}")
    print("="*80)
    print()
    print("üôè –î—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è: –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ AI")
    print(f"‚è±Ô∏è  –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{epochs * len(tokenized_dataset) // (batch_size * 4) // 60} –º–∏–Ω—É—Ç")
    print()
    
    try:
        trainer.train()
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")
        
        print("\n" + "="*80)
        print(f"‚úÖ –°–§–ï–†–ê {sphere} –û–ë–£–ß–ï–ù–ê!")
        print("="*80)
        print(f"üìÅ {output_dir}")
        print("üôè –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="–§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ —Å—Ñ–µ—Ä —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏")
    parser.add_argument("--sphere", required=True, choices=["073", "074", "075", "076", "077", "078"],
                       help="–ù–æ–º–µ—Ä —Å—Ñ–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("--dataset", default="../datasets/developers_108_perfect.jsonl",
                       help="–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument("--output", default="../models",
                       help="–ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
    parser.add_argument("--base-model", default="nativemind/shridhar_8k_multimodal",
                       help="–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å")
    parser.add_argument("--epochs", type=int, default=3, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
    parser.add_argument("--batch-size", type=int, default=2, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--learning-rate", type=float, default=2e-4, help="Learning rate")
    parser.add_argument("--use-8bit", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 8-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ")
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    output_dir = f"{args.output}/sphere_{args.sphere}_shridhar"
    os.makedirs(output_dir, exist_ok=True)
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    success = finetune_developer_sphere(
        sphere=args.sphere,
        dataset_path=args.dataset,
        output_dir=output_dir,
        base_model=args.base_model,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        use_8bit=args.use_8bit
    )
    
    if success:
        print(f"\nüéâ –°—Ñ–µ—Ä–∞ {args.sphere} —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
        sys.exit(0)
    else:
        print(f"\nüí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å—Ñ–µ—Ä—ã {args.sphere}")
        sys.exit(1)

if __name__ == "__main__":
    main()
