#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –¥–ª—è M4 MacBook Pro –∑–∞ –Ω–æ—á—å

–°—Ç—Ä–∞—Ç–µ–≥–∏—è:
- –ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å (TinyLlama 1.1B)
- LoRA (rank=8) –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- MPS acceleration (Apple Silicon)
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è ~2 —á–∞—Å–∞ –Ω–∞ —Å—Ñ–µ—Ä—É

¬© 2025 NativeMind - NativeMindNONC License
"""

import os
import sys
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType

def finetune_sphere_m4(
    sphere,
    dataset_path="../datasets/combined_legal_training.jsonl",
    output_dir=None,
    base_model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    max_steps=800,
    batch_size=8
):
    """
    –ë—ã—Å—Ç—Ä—ã–π —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –¥–ª—è M4
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - TinyLlama (1.1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
    - LoRA rank=8
    - MPS (Apple Silicon)
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —à–∞–≥–∏ (800 = ~2 —á–∞—Å–∞)
    """
    
    sphere_names = {
        "047": "–°–õ–ï–î–û–í–ê–¢–ï–õ–¨",
        "048": "–ü–†–û–ö–£–†–û–†",
        "049": "–°–£–î–¨–Ø"
    }
    
    if output_dir is None:
        output_dir = f"../models/sphere_{sphere}_m4_overnight"
    
    print("="*80)
    print(f"‚öñÔ∏è  –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –°—Ñ–µ—Ä—ã {sphere}: {sphere_names[sphere]}")
    print("="*80)
    print()
    print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: Apple M4 (MPS)")
    print(f"üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model}")
    print(f"üìö –î–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
    print(f"üéØ –ú–∞–∫—Å–∏–º—É–º —à–∞–≥–æ–≤: {max_steps} (~2 —á–∞—Å–∞)")
    print(f"üíæ –í—ã—Ö–æ–¥: {output_dir}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    print("   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    print("   üçé –ò—Å–ø–æ–ª—å–∑—É–µ–º Apple Silicon MPS")
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.float32,  # MPS –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å FP32
        trust_remote_code=True,
        use_cache=False,  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
    )
    
    # Gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    model.gradient_checkpointing_enable()
    
    model = model.to("mps")
    
    print("   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ MPS")
    print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.num_parameters():,}")
    
    # 3. LoRA - –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ 0.1% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!
    print("\nüîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)...")
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π rank –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],  # –¢–æ–ª—å–∫–æ 2 –º–æ–¥—É–ª—è
        bias="none"
    )
    
    model = get_peft_model(model, lora_config)
    
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    
    print(f"   ‚úÖ LoRA –ø—Ä–∏–º–µ–Ω–µ–Ω")
    print(f"   üéØ –û–±—É—á–∞–µ–º—ã—Ö: {trainable:,} ({100*trainable/total:.2f}%)")
    print(f"   üìä –í—Å–µ–≥–æ: {total:,}")
    
    # 4. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset = load_dataset('json', data_files=dataset_path, split='train')
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å—Ñ–µ—Ä–µ (–∏–ª–∏ –±–µ—Ä–µ–º –≤—Å–µ –¥–ª—è –¥–∏–∞–ª–æ–≥–∞)
    role_map = {
        "047": "sphere_047_investigator",
        "048": "sphere_048_prosecutor",
        "049": "sphere_049_judge"
    }
    
    # –ë–µ—Ä–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å—Ñ–µ—Ä—ã + –æ–±—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –¥–∏–∞–ª–æ–≥–∞
    filtered = dataset.filter(
        lambda x: x.get('role') == role_map.get(sphere) or x.get('sphere') == 'general'
    )
    
    print(f"   üìä –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –°—Ñ–µ—Ä—ã {sphere}: {len(filtered)}")
    
    # 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    print("\nüî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    
    def format_prompt(example):
        """–§–æ—Ä–º–∞—Ç Llama/Alpaca"""
        instruction = example.get('instruction', '')
        input_text = example.get('input', '')
        output_text = example.get('output', '')
        
        if input_text:
            prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output_text}"
        else:
            prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output_text}"
        
        return tokenizer(
            prompt,
            truncation=True,
            max_length=1024,  # –ö–æ—Ä–æ—á–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            padding="max_length"
        )
    
    tokenized = filtered.map(
        format_prompt,
        remove_columns=filtered.column_names,
        desc="–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
    )
    
    print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(tokenized)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # 6. Training arguments - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è M4!
    print("\n‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è M4)...")
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=1,  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏!
        gradient_accumulation_steps=16,  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 16
        learning_rate=5e-4,
        warmup_steps=50,
        logging_steps=20,
        save_steps=300,
        save_total_limit=1,  # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π checkpoint
        max_steps=max_steps,
        fp16=False,
        bf16=False,
        use_cpu=False,
        optim="adamw_torch",
        logging_dir=f"{output_dir}/logs",
        report_to=[],
        remove_unused_columns=False,
        gradient_checkpointing=True,  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏!
        dataloader_num_workers=0,  # –ë–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    )
    
    print("   ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"      –≠–ø–æ—Ö: 1")
    print(f"      Batch size: {batch_size}")
    print(f"      –ì—Ä–∞–¥–∏–µ–Ω—Ç accumulation: 2")
    print(f"      Max steps: {max_steps}")
    print(f"      –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch: {batch_size * 2}")
    
    # 7. Trainer
    print("\nüèóÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ Trainer...")
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized,
        data_collator=data_collator
    )
    
    print("   ‚úÖ Trainer –≥–æ—Ç–æ–≤")
    
    # 8. –û–ë–£–ß–ï–ù–ò–ï
    print(f"\n{'='*80}")
    print(f"üèãÔ∏è  –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –°—Ñ–µ—Ä—ã {sphere}: {sphere_names[sphere]}")
    print(f"{'='*80}")
    print()
    print("üôè –î—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è: –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ AI")
    print(f"‚è±Ô∏è  –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~2 —á–∞—Å–∞")
    print()
    
    try:
        trainer.train()
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # README
    readme = f"""# {sphere_names[sphere]} - –°—Ñ–µ—Ä–∞ {sphere} (M4 Overnight)

**–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:** {base_model}  
**–û–±—É—á–µ–Ω–æ –Ω–∞:** M4 MacBook Pro –∑–∞ ~2 —á–∞—Å–∞  
**–ú–µ—Ç–æ–¥:** LoRA (rank=8)  
**–î–∞—Ç–∞—Å–µ—Ç:** –†–µ–∞–ª—å–Ω–æ–µ —É–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ + Alpaca + Kene

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base = AutoModelForCausalLM.from_pretrained("{base_model}")
model = PeftModel.from_pretrained(base, "{output_dir}")
tokenizer = AutoTokenizer.from_pretrained("{output_dir}")

prompt = "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç..."
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=512)
print(tokenizer.decode(outputs[0]))
```

**‚öñÔ∏è –ò—Å—Ç–∏–Ω–∞ –≤–æ—Å—Ç–æ—Ä–∂–µ—Å—Ç–≤—É–µ—Ç! üïâÔ∏è**
"""
    
    with open(os.path.join(output_dir, "README.md"), 'w', encoding='utf-8') as f:
        f.write(readme)
    
    print(f"\n{'='*80}")
    print(f"‚úÖ –°–§–ï–†–ê {sphere} –û–ë–£–ß–ï–ù–ê!")
    print(f"{'='*80}")
    print(f"\nüìÅ {output_dir}")
    print(f"üôè –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...")
    
    return True

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--sphere", required=True, choices=["047", "048", "049"])
    parser.add_argument("--dataset", default="../datasets/combined_legal_training.jsonl")
    parser.add_argument("--output", default=None)
    parser.add_argument("--steps", type=int, default=800)
    parser.add_argument("--batch-size", type=int, default=8)
    
    args = parser.parse_args()
    
    finetune_sphere_m4(
        sphere=args.sphere,
        dataset_path=args.dataset,
        output_dir=args.output,
        max_steps=args.steps,
        batch_size=args.batch_size
    )

