#!/usr/bin/env python3
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ç–æ–º–æ–≤ —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞ –í–∏–∫—Ç–æ—Ä–∞

–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç 107 —Ç–æ–º–æ–≤ PDF ‚Üí –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç ‚Üí —Å–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥–∞
–°—Ñ–µ—Ä—ã 047 (–°–õ–ï–î–û–í–ê–¢–ï–õ–¨), 048 (–ü–†–û–ö–£–†–û–†), 049 (–°–£–î–¨–Ø)

¬© 2025 NativeMind - NativeMindNONC License
"""

import os
import sys
import json
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from ocr_engine import OCREngine

def prepare_legal_dataset(
    pdf_dir="/Volumes/MOZGACH/Advokat/Ugolovka/Viktor/–¢–æ–º–∞",
    output_dir="../datasets/legal_case_viktor",
    sample_pages=10,  # –°—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    max_tomes=None  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ç–æ–º–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (None = –≤—Å–µ)
):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ç–æ–º–æ–≤ –¥–µ–ª–∞
    
    Args:
        pdf_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–æ–º–∞–º–∏ PDF
        output_dir: –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
        sample_pages: –°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–º–∞
        max_tomes: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–º–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∞)
    """
    print("="*80)
    print("üìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ç–æ–º–æ–≤ —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞")
    print("="*80)
    print()
    print("üôè –î—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è: –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ —á–µ—Ä–µ–∑ AI")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
    if not os.path.exists(pdf_dir):
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {pdf_dir}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∏—Å–∫ MOZGACH –ø–æ–¥–∫–ª—é—á–µ–Ω")
        return None
    
    # –°–æ–∑–¥–∞–µ–º OCR –¥–≤–∏–∂–æ–∫
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR –¥–≤–∏–∂–∫–∞...")
    try:
        ocr = OCREngine(languages=['rus', 'eng'])
        print("   ‚úÖ OCR –≥–æ—Ç–æ–≤")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ OCR: {e}")
        print("   üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pytesseract PyMuPDF pdf2image")
        return None
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ PDF
    pdf_files = sorted(Path(pdf_dir).glob("*.pdf"))
    if max_tomes:
        pdf_files = pdf_files[:max_tomes]
    
    print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–º–æ–≤: {len(pdf_files)}")
    print(f"   –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ {sample_pages} —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–∞–∂–¥–æ–≥–æ")
    print()
    
    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs(output_dir, exist_ok=True)
    
    dataset = []
    processed_tomes = 0
    total_pages = 0
    
    for i, pdf_path in enumerate(pdf_files):
        print(f"üìÑ [{i+1}/{len(pdf_files)}] {pdf_path.name}")
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            texts = ocr.extract_text_from_pdf(
                str(pdf_path),
                start_page=0,
                end_page=sample_pages,
                preserve_layout=True
            )
            
            pages_processed = 0
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            for page_num, text in texts.items():
                if len(text.strip()) < 100:
                    continue
                
                # –ß–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
                text_clean = text.strip()[:2000]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤
                
                # –°–§–ï–†–ê 047: –°–õ–ï–î–û–í–ê–¢–ï–õ–¨
                dataset.append({
                    "instruction": "–¢—ã - –°–õ–ï–î–û–í–ê–¢–ï–õ–¨ (–°—Ñ–µ—Ä–∞ 047). –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –±–µ—Å–ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤.",
                    "input": f"–¢–æ–º {pdf_path.stem}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num+1}:\n\n{text_clean}",
                    "output": f"–ö–∞–∫ —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–æ–∫—É–º–µ–Ω—Ç. –ü—Ä–æ–≤–µ—Ä—è—é: 1) –ü–æ–ª–Ω–æ—Ç—É –∏–∑–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤, 2) –ù–∞–ª–∏—á–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π, 3) –ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—É—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è, 4) –û–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–µ–ª–∞.",
                    "role": "sphere_047_investigator",
                    "sphere": "047",
                    "source_file": str(pdf_path.name),
                    "page": page_num + 1
                })
                
                # –°–§–ï–†–ê 048: –ü–†–û–ö–£–†–û–† (–ö–õ–Æ–ß–ï–í–ê–Ø!)
                dataset.append({
                    "instruction": "–¢—ã - –ü–†–û–ö–£–†–û–† (–°—Ñ–µ—Ä–∞ 048). –¢–≤–æ—è –¥—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–ø–∏–ø–∞—Å—Ç–∞ –∫–∞–∫ —Å–∏–º–ø—Ç–æ–º–∞ –Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏. –ü—Ä–æ–≤–µ—Ä—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏.",
                    "input": f"–¢–æ–º {pdf_path.stem}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num+1}:\n\n{text_clean}",
                    "output": f"–ö–∞–∫ –ø—Ä–æ–∫—É—Ä–æ—Ä, –ø—Ä–æ–≤–æ–∂—É –Ω–∞–¥–∑–æ—Ä –∑–∞ –∑–∞–∫–æ–Ω–Ω–æ—Å—Ç—å—é. –ü—Ä–æ–≤–µ—Ä—è—é: 1) –ù–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–æ–ø–∏–ø–∞—Å—Ç–∞ —Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ —Å–ª–µ–¥—Å—Ç–≤–∏—è, 2) –ù–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏, 3) –§–æ—Ä–º–∞–ª—å–Ω—ã–π –ª–∏ –ø–æ–¥—Ö–æ–¥ –∫ –¥–µ–ª—É, 4) –°–ª—É–∂–∏—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –∏—Å—Ç–∏–Ω–µ. –í–ê–ñ–ù–û: –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —Å–∏–º–ø—Ç–æ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –≤–æ–∑–º–æ–∂–Ω–æ–π –∫–æ—Ä—Ä—É–ø—Ü–∏–∏.",
                    "role": "sphere_048_prosecutor",
                    "sphere": "048",
                    "source_file": str(pdf_path.name),
                    "page": page_num + 1
                })
                
                # –°–§–ï–†–ê 049: –°–£–î–¨–Ø
                dataset.append({
                    "instruction": "–¢—ã - –°–£–î–¨–Ø (–°—Ñ–µ—Ä–∞ 049). –¢–≤–æ—è –¥—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è - –≤—ã–Ω–µ—Å–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –û—Ü–µ–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–ª—É–∂–µ–Ω–∏—è –∏—Å—Ç–∏–Ω–µ.",
                    "input": f"–¢–æ–º {pdf_path.stem}, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num+1}:\n\n{text_clean}",
                    "output": f"–ö–∞–∫ —Å—É–¥—å—è, –æ—Ü–µ–Ω–∏–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç –±–µ—Å–ø—Ä–∏—Å—Ç—Ä–∞—Å—Ç–Ω–æ. –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é: 1) –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, 2) –ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—É—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å, 3) –†–∞–±–æ—Ç—É –ø—Ä–æ–∫—É—Ä–æ—Ä–∞ –∏ —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è, 4) –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏. –¶–µ–ª—å - –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏, –∞ –Ω–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞–º.",
                    "role": "sphere_049_judge",
                    "sphere": "049",
                    "source_file": str(pdf_path.name),
                    "page": page_num + 1
                })
                
                pages_processed += 1
                total_pages += 1
            
            processed_tomes += 1
            print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {pages_processed} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")
            continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∂–¥—ã–µ 10 —Ç–æ–º–æ–≤
        if (i + 1) % 10 == 0:
            print(f"\nüíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...")
            _save_dataset(dataset, output_dir, partial=True)
    
    print(f"\n{'='*80}")
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"{'='*80}")
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–º–æ–≤: {processed_tomes}")
    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
    print()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    _save_dataset(dataset, output_dir, partial=False)
    
    return dataset


def _save_dataset(dataset, output_dir, partial=False):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    suffix = "_partial" if partial else ""
    
    # JSONL —Ñ–æ—Ä–º–∞—Ç
    output_file = os.path.join(output_dir, f"legal_dataset{suffix}.jsonl")
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_file}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        "total_examples": len(dataset),
        "examples_by_sphere": {
            "047_investigator": len([d for d in dataset if d['sphere'] == '047']),
            "048_prosecutor": len([d for d in dataset if d['sphere'] == '048']),
            "049_judge": len([d for d in dataset if d['sphere'] == '049'])
        },
        "tomes_processed": len(set([d['source_file'] for d in dataset])),
        "avg_examples_per_tome": len(dataset) / len(set([d['source_file'] for d in dataset])) if dataset else 0
    }
    
    stats_file = os.path.join(output_dir, f"stats{suffix}.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats_file}")
    print(f"      - –°—Ñ–µ—Ä–∞ 047 (–°–õ–ï–î–û–í–ê–¢–ï–õ–¨): {stats['examples_by_sphere']['047_investigator']}")
    print(f"      - –°—Ñ–µ—Ä–∞ 048 (–ü–†–û–ö–£–†–û–†): {stats['examples_by_sphere']['048_prosecutor']}")
    print(f"      - –°—Ñ–µ—Ä–∞ 049 (–°–£–î–¨–Ø): {stats['examples_by_sphere']['049_judge']}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ç–æ–º–æ–≤ —É–≥–æ–ª–æ–≤–Ω–æ–≥–æ –¥–µ–ª–∞"
    )
    parser.add_argument(
        "--pdf-dir",
        default="/Volumes/MOZGACH/Advokat/Ugolovka/Viktor/–¢–æ–º–∞",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å PDF —Ç–æ–º–∞–º–∏"
    )
    parser.add_argument(
        "--output-dir",
        default="../datasets/legal_case_viktor",
        help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç"
    )
    parser.add_argument(
        "--pages",
        type=int,
        default=10,
        help="–°—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–º–∞"
    )
    parser.add_argument(
        "--max-tomes",
        type=int,
        default=None,
        help="–ú–∞–∫—Å–∏–º—É–º —Ç–æ–º–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∞)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (–ø–µ—Ä–≤—ã–µ 3 —Ç–æ–º–∞, 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)"
    )
    
    args = parser.parse_args()
    
    if args.test:
        print("\nüß™ –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú")
        prepare_legal_dataset(
            pdf_dir=args.pdf_dir,
            output_dir=args.output_dir,
            sample_pages=2,
            max_tomes=3
        )
    else:
        prepare_legal_dataset(
            pdf_dir=args.pdf_dir,
            output_dir=args.output_dir,
            sample_pages=args.pages,
            max_tomes=args.max_tomes
        )








