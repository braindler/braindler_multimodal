"""
–û—Å–Ω–æ–≤–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è Braindler & Mozgach

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    Vision Encoder (CLIP) ‚Üí Projection ‚Üí Language Model (Braindler/Mozgach)

¬© 2025 NativeMind - NativeMindNONC License
"""

import torch
import torch.nn as nn
from typing import Optional, Union, List
from PIL import Image
from transformers import AutoModelForCausalLM, AutoTokenizer
from .vision_encoder import VisionEncoder
from .projection import ProjectionLayer


class MultimodalBraindler(nn.Module):
    """
    –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è Braindler
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –¢–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
    """
    
    def __init__(
        self,
        language_model_name: str = "nativemind/braindler_final_model",
        vision_model_name: str = "openai/clip-vit-large-patch14",
        device: str = "auto",
    ):
        super().__init__()
        
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MultimodalBraindler...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
            elif torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device
            
        print(f"   üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º vision encoder
        print(f"   üëÅÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ Vision Encoder: {vision_model_name}")
        self.vision_encoder = VisionEncoder(vision_model_name)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å
        print(f"   üß† –ó–∞–≥—Ä—É–∑–∫–∞ Language Model: {language_model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(language_model_name)
        self.language_model = AutoModelForCausalLM.from_pretrained(
            language_model_name,
            torch_dtype=torch.float32 if self.device == "cpu" else torch.float16,
            device_map=self.device if self.device != "mps" else None,
        )
        
        if self.device == "mps":
            self.language_model = self.language_model.to("mps")
        
        # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π: CLIP embedding ‚Üí Language model embedding
        vision_dim = self.vision_encoder.get_embedding_dim()
        language_dim = self.language_model.config.hidden_size
        
        print(f"   üîó –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ —Å–ª–æ—è: {vision_dim} ‚Üí {language_dim}")
        self.projection = ProjectionLayer(vision_dim, language_dim)
        self.projection = self.projection.to(self.device)
        
        # Pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.language_model.config.pad_token_id = self.language_model.config.eos_token_id
        
        print("   ‚úÖ MultimodalBraindler –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    def encode_image(self, image: Union[str, Image.Image]) -> torch.Tensor:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥
        
        Args:
            image: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏–ª–∏ PIL.Image
            
        Returns:
            Tensor —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        # –ü–æ–ª—É—á–∞–µ–º CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥
        vision_embedding = self.vision_encoder.encode(image)
        vision_embedding = vision_embedding.to(self.device)
        
        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        language_embedding = self.projection(vision_embedding)
        
        return language_embedding
    
    def chat(
        self,
        prompt: str,
        image: Optional[Union[str, Image.Image]] = None,
        max_length: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> str:
        """
        –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π —á–∞—Ç
        
        Args:
            prompt: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            image: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_p: Top-p sampling
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if image is not None:
            # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
            image_embedding = self.encode_image(image)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_prefix = "[–ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï] "
            full_prompt = image_prefix + prompt
        else:
            # –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
            full_prompt = prompt
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            full_prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.no_grad():
            outputs = self.language_model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
        if response.startswith(full_prompt):
            response = response[len(full_prompt):].strip()
        
        return response
    
    def batch_encode_images(self, images: List[Union[str, Image.Image]]) -> torch.Tensor:
        """
        –ü–∞–∫–µ—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        
        Args:
            images: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            
        Returns:
            Tensor —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        embeddings = []
        
        for image in images:
            emb = self.encode_image(image)
            embeddings.append(emb)
        
        return torch.stack(embeddings)
    
    @classmethod
    def from_pretrained(cls, model_path: str, **kwargs):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –≠–∫–∑–µ–º–ø–ª—è—Ä MultimodalBraindler
        """
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É fine-tuned projection layer
        return cls(language_model_name=model_path, **kwargs)
    
    def save_pretrained(self, save_path: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        import os
        os.makedirs(save_path, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å
        self.language_model.save_pretrained(os.path.join(save_path, "language_model"))
        self.tokenizer.save_pretrained(os.path.join(save_path, "language_model"))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π
        torch.save(
            self.projection.state_dict(),
            os.path.join(save_path, "projection.pt")
        )
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}")


class MultimodalMozgach(MultimodalBraindler):
    """
    –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è Mozgach
    
    –†–∞—Å—à–∏—Ä—è–µ—Ç MultimodalBraindler —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
    –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    
    def __init__(
        self,
        language_model_name: str = "nativemind/mozgach_full_trained_model",
        vision_model_name: str = "openai/clip-vit-large-patch14",
        device: str = "auto",
    ):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MultimodalMozgach...")
        super().__init__(language_model_name, vision_model_name, device)
        print("   ‚úÖ MultimodalMozgach –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    def analyze_code_screenshot(self, image: Union[str, Image.Image]) -> str:
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –∫–æ–¥–∞
        
        Args:
            image: –°–∫—Ä–∏–Ω—à–æ—Ç –∫–æ–¥–∞
            
        Returns:
            –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞
        """
        prompt = "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–¥ –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –û–ø–∏—à–∏, —á—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è."
        return self.chat(prompt, image=image)
    
    def analyze_diagram(self, image: Union[str, Image.Image]) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–≥—Ä–∞–º–º –∏ —Å—Ö–µ–º
        
        Args:
            image: –î–∏–∞–≥—Ä–∞–º–º–∞ –∏–ª–∏ —Å—Ö–µ–º–∞
            
        Returns:
            –û–ø–∏—Å–∞–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã
        """
        prompt = "–û–ø–∏—à–∏ —ç—Ç—É –¥–∏–∞–≥—Ä–∞–º–º—É –∏–ª–∏ —Å—Ö–µ–º—É. –û–±—ä—è—Å–Ω–∏ –µ—ë —Å–º—ã—Å–ª –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã."
        return self.chat(prompt, image=image)



