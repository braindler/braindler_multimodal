"""
–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–î—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è: –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏
–ö–ª—é—á–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–ø–∏–ø–∞—Å—Ç–∞ –∫–∞–∫ —Å–∏–º–ø—Ç–æ–º–∞ –Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏

–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ü–†–ê–í–ò–õ–û –∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ "–°–¥–µ–ª–∞–π, –°—Ç–∞—Ä–µ—Ü!"

¬© 2025 NativeMind - NativeMindNONC License
"""

import os
import difflib
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from Levenshtein import ratio as levenshtein_ratio
from fuzzywuzzy import fuzz
from .ocr_engine import OCREngine


@dataclass
class CopyPasteResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–æ–ø–∏–ø–∞—Å—Ç–∞"""
    text_similarity: float  # –ü—Ä–æ—Ü–µ–Ω—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (0-100)
    visual_similarity: float  # –í–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0-100)
    suspicious_blocks: List[Tuple[str, str]]  # [(–±–ª–æ–∫_–ø—Ä–æ–∫—É—Ä–æ—Ä–∞, –±–ª–æ–∫_—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è)]
    identical_sections: List[str]  # –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
    suspicious_patterns: List[str]  # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    spiritual_verdict: str  # –í–µ—Ä–¥–∏–∫—Ç —Å –¥—É—Ö–æ–≤–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è


@dataclass
class LegalCase:
    """–£–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ"""
    case_name: str
    prosecutor_documents: Dict[str, Dict[int, str]]  # {filename: {page: text}}
    investigator_documents: Dict[str, Dict[int, str]]
    metadata: Dict[str, any]  # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–¥–∞—Ç—ã, –ø–æ–¥–ø–∏—Å–∏ –∏ —Ç.–¥.)


class LegalDocumentAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    –î—É—Ö–æ–≤–Ω–∞—è —Ü–µ–ª—å:
        "–ù–∞–º –≤–∞–∂–Ω–æ –ø–æ–Ω—è—Ç—å –∏—Å—Ç–∏–Ω—É –∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è, 
         —Ç–æ –µ—Å—Ç—å —Å –¥—É—Ö–æ–≤–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è."
    
    –ö–ª—é—á–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è:
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–ø–∏–ø–∞—Å—Ç–∞ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –ø—Ä–æ–∫—É—Ä–æ—Ä–∞ –∏ —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è
        –∫–∞–∫ —Å–∏–º–ø—Ç–æ–º–∞ –≤–æ–∑–º–æ–∂–Ω–æ–π –Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏
    """
    
    def __init__(self, use_easyocr: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            use_easyocr: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å EasyOCR –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        """
        print("‚öñÔ∏è  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LegalDocumentAnalyzer...")
        print("   üôè –î—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è: –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏")
        
        # OCR –¥–≤–∏–∂–æ–∫
        self.ocr = OCREngine(languages=['rus', 'eng'], use_easyocr=use_easyocr)
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        self.SUSPICIOUS_THRESHOLD = 70.0  # % —Å—Ö–æ–¥—Å—Ç–≤–∞
        self.IDENTICAL_THRESHOLD = 95.0   # % –¥–ª—è –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        
        print("   ‚úÖ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ –∫ —Å–ª—É–∂–µ–Ω–∏—é –∏—Å—Ç–∏–Ω–µ")
    
    def process_case(
        self,
        prosecutor_docs: List[str],
        investigator_docs: List[str],
        case_name: str = "–£–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ",
    ) -> LegalCase:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–µ–ª–∞
        
        Args:
            prosecutor_docs: –°–ø–∏—Å–æ–∫ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–∫—É—Ä–æ—Ä–∞
            investigator_docs: –°–ø–∏—Å–æ–∫ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è
            case_name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–µ–ª–∞
            
        Returns:
            LegalCase —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        print(f"\nüìö –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ–ª–∞: {case_name}")
        print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ–∫—É—Ä–æ—Ä–∞: {len(prosecutor_docs)}")
        print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ã —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è: {len(investigator_docs)}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ–∫—É—Ä–æ—Ä–∞
        print("\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–∫—É—Ä–æ—Ä–∞...")
        prosecutor_data = self.ocr.batch_process_pdfs(prosecutor_docs)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è
        print("\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è...")
        investigator_data = self.ocr.batch_process_pdfs(investigator_docs)
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–µ–ª–∞
        case = LegalCase(
            case_name=case_name,
            prosecutor_documents=prosecutor_data,
            investigator_documents=investigator_data,
            metadata={
                'prosecutor_files': prosecutor_docs,
                'investigator_files': investigator_docs,
            }
        )
        
        print(f"\n‚úÖ –î–µ–ª–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
        return case
    
    def detect_copypaste(
        self,
        case: LegalCase,
        block_size: int = 500,  # –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—Å–∏–º–≤–æ–ª–æ–≤)
    ) -> CopyPasteResult:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∫–æ–ø–∏–ø–∞—Å—Ç –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        
        –î—É—Ö–æ–≤–Ω–∞—è –º–∏—Å—Å–∏—è:
            "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ª–µ–Ω—å, —ç—Ç–æ —Å–∏–º–ø—Ç–æ–º –±–æ–ª–µ–µ 
             –≥–ª—É–±–æ–∫–æ–π –ø—Ä–æ–±–ª–µ–º—ã: –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏, —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ 
             –ø–æ–¥—Ö–æ–¥–∞ –∫ —Å—É–¥—å–±–∞–º –ª—é–¥–µ–π, –≤–æ–∑–º–æ–∂–Ω–æ–π –∫–æ—Ä—Ä—É–ø—Ü–∏–∏."
        
        Args:
            case: –£–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ
            block_size: –†–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            CopyPasteResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –∫–æ–ø–∏–ø–∞—Å—Ç–∞: {case.case_name}")
        print("   üôè –°–ª—É–∂–µ–Ω–∏–µ –∏—Å—Ç–∏–Ω–µ —á–µ—Ä–µ–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
        prosecutor_text = self._merge_all_texts(case.prosecutor_documents)
        investigator_text = self._merge_all_texts(case.investigator_documents)
        
        # 1. –û–±—â–µ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        text_similarity = self._calculate_text_similarity(
            prosecutor_text,
            investigator_text
        )
        
        print(f"   üìä –û–±—â–µ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {text_similarity:.2f}%")
        
        # 2. –ü–æ–∏—Å–∫ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        identical_sections = self._find_identical_sections(
            prosecutor_text,
            investigator_text,
            block_size
        )
        
        print(f"   üî¥ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –±–ª–æ–∫–æ–≤: {len(identical_sections)}")
        
        # 3. –ü–æ–∏—Å–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤ (—á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)
        suspicious_blocks = self._find_suspicious_blocks(
            prosecutor_text,
            investigator_text,
            block_size
        )
        
        print(f"   ‚ö†Ô∏è  –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤: {len(suspicious_blocks)}")
        
        # 4. –ê–Ω–∞–ª–∏–∑ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        suspicious_patterns = self._analyze_patterns(
            prosecutor_text,
            investigator_text
        )
        
        # 5. –î—É—Ö–æ–≤–Ω—ã–π –≤–µ—Ä–¥–∏–∫—Ç
        spiritual_verdict = self._make_spiritual_verdict(
            text_similarity,
            len(identical_sections),
            len(suspicious_blocks),
            suspicious_patterns
        )
        
        print(f"\n‚öñÔ∏è  –î—É—Ö–æ–≤–Ω—ã–π –≤–µ—Ä–¥–∏–∫—Ç: {spiritual_verdict}")
        
        return CopyPasteResult(
            text_similarity=text_similarity,
            visual_similarity=0.0,  # TODO: –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            suspicious_blocks=suspicious_blocks,
            identical_sections=identical_sections,
            suspicious_patterns=suspicious_patterns,
            spiritual_verdict=spiritual_verdict,
        )
    
    def _merge_all_texts(self, documents: Dict[str, Dict[int, str]]) -> str:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        all_texts = []
        
        for filename, pages in documents.items():
            for page_num in sorted(pages.keys()):
                all_texts.append(pages[page_num])
        
        return '\n\n'.join(all_texts)
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤:
        - SequenceMatcher (—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
        - Levenshtein (–ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
        - FuzzyWuzzy (–Ω–µ—á–µ—Ç–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
        """
        # 1. SequenceMatcher
        seq_ratio = difflib.SequenceMatcher(None, text1, text2).ratio()
        
        # 2. Levenshtein (–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤)
        if len(text1) < 10000 and len(text2) < 10000:
            lev_ratio = levenshtein_ratio(text1, text2)
        else:
            lev_ratio = seq_ratio  # Fallback –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        # 3. FuzzyWuzzy (token-based)
        fuzzy_ratio = fuzz.token_sort_ratio(text1, text2) / 100.0
        
        # –°—Ä–µ–¥–Ω–µ–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ
        similarity = (seq_ratio * 0.4 + lev_ratio * 0.3 + fuzzy_ratio * 0.3) * 100
        
        return similarity
    
    def _find_identical_sections(
        self,
        text1: str,
        text2: str,
        block_size: int
    ) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        identical = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–ª–æ–∫–∏
        blocks1 = self._split_into_blocks(text1, block_size)
        blocks2 = self._split_into_blocks(text2, block_size)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º
        for block1 in blocks1:
            for block2 in blocks2:
                similarity = fuzz.ratio(block1, block2)
                
                if similarity >= self.IDENTICAL_THRESHOLD:
                    if block1 not in identical:
                        identical.append(block1)
        
        return identical
    
    def _find_suspicious_blocks(
        self,
        text1: str,
        text2: str,
        block_size: int
    ) -> List[Tuple[str, str]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–µ –±–ª–æ–∫–∏"""
        suspicious = []
        
        blocks1 = self._split_into_blocks(text1, block_size)
        blocks2 = self._split_into_blocks(text2, block_size)
        
        for block1 in blocks1:
            for block2 in blocks2:
                similarity = fuzz.ratio(block1, block2)
                
                if self.SUSPICIOUS_THRESHOLD <= similarity < self.IDENTICAL_THRESHOLD:
                    suspicious.append((block1, block2))
        
        return suspicious
    
    def _split_into_blocks(self, text: str, block_size: int) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –±–ª–æ–∫–∏"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
        paragraphs = text.split('\n\n')
        
        blocks = []
        current_block = ""
        
        for para in paragraphs:
            if len(current_block) + len(para) < block_size:
                current_block += para + '\n\n'
            else:
                if current_block.strip():
                    blocks.append(current_block.strip())
                current_block = para + '\n\n'
        
        if current_block.strip():
            blocks.append(current_block.strip())
        
        return blocks
    
    def _analyze_patterns(self, text1: str, text2: str) -> List[str]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        patterns = []
        
        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏/–æ—à–∏–±–∫–∏
        # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –æ–ø–µ—á–∞—Ç–æ–∫
        
        # –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
        common_phrases = self._find_common_phrases(text1, text2)
        if len(common_phrases) > 50:
            patterns.append(
                f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(common_phrases)} –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫"
            )
        
        # –û–¥–∏–Ω–∞–∫–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        if self._similar_structure(text1, text2):
            patterns.append("–ò–¥–µ–Ω—Ç–∏—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return patterns
    
    def _find_common_phrases(self, text1: str, text2: str, min_length: int = 30) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ SequenceMatcher
        matcher = difflib.SequenceMatcher(None, text1, text2)
        matches = matcher.get_matching_blocks()
        
        common = []
        for match in matches:
            if match.size >= min_length:
                phrase = text1[match.a:match.a + match.size]
                common.append(phrase)
        
        return common
    
    def _similar_structure(self, text1: str, text2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        para1 = len(text1.split('\n\n'))
        para2 = len(text2.split('\n\n'))
        
        if para1 == 0 or para2 == 0:
            return False
        
        ratio = min(para1, para2) / max(para1, para2)
        return ratio > 0.9
    
    def _make_spiritual_verdict(
        self,
        similarity: float,
        identical_count: int,
        suspicious_count: int,
        patterns: List[str],
    ) -> str:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –≤–µ—Ä–¥–∏–∫—Ç —Å –¥—É—Ö–æ–≤–Ω–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è
        
        "–ù–∞–º –≤–∞–∂–Ω–æ –ø–æ–Ω—è—Ç—å –∏—Å—Ç–∏–Ω—É –∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è"
        """
        if similarity >= 80.0:
            verdict = (
                "üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û–±–Ω–∞—Ä—É–∂–µ–Ω –º–∞—Å—Å–æ–≤—ã–π –∫–æ–ø–∏–ø–∞—Å—Ç! "
                "–≠—Ç–æ –∑–Ω–∞–∫ –∫ —Ç–æ–º—É, —á—Ç–æ –¥–µ–ª–æ –≤–æ–∑–º–æ–∂–Ω–æ –∫—É–ø–ª–µ–Ω–Ω–æ–µ –∏ –Ω—É–∂–µ–Ω "
                "—É—Å–∏–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å. –ü—Ä–æ–∫—É—Ä–æ—Ä –Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª —Å–≤–æ—é —Å–≤—è—â–µ–Ω–Ω—É—é "
                "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏."
            )
        elif similarity >= 60.0:
            verdict = (
                "‚ö†Ô∏è  –°–ï–†–¨–ï–ó–ù–û–ï –ü–û–î–û–ó–†–ï–ù–ò–ï: –í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —É–∫–∞–∑—ã–≤–∞–µ—Ç "
                "–Ω–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥. –¢—Ä–µ–±—É–µ—Ç—Å—è —Ç—â–∞—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç "
                "–æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."
            )
        elif similarity >= 40.0:
            verdict = (
                "‚ö° –£–ú–ï–†–ï–ù–ù–û–ï –ü–û–î–û–ó–†–ï–ù–ò–ï: –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç "
                "–±—ã—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏, –Ω–æ "
                "—Ç—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è."
            )
        else:
            verdict = (
                "‚úÖ –ù–ï–ó–ê–í–ò–°–ò–ú–ê–Ø –ü–†–û–í–ï–†–ö–ê: –î–æ–∫—É–º–µ–Ω—Ç—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è, "
                "—á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é —Ä–∞–±–æ—Ç—É –ø—Ä–æ–∫—É—Ä–æ—Ä–∞ –∏ —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è. "
                "–ò—Å—Ç–∏–Ω–∞ –≤–æ—Å—Ç–æ—Ä–∂–µ—Å—Ç–≤—É–µ—Ç."
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏
        if identical_count > 0:
            verdict += f"\n\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {identical_count} –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö –±–ª–æ–∫–æ–≤."
        
        if suspicious_count > 0:
            verdict += f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {suspicious_count} –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –±–ª–æ–∫–æ–≤."
        
        if patterns:
            verdict += f"\n\n–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:\n" + '\n'.join(f"‚Ä¢ {p}" for p in patterns)
        
        return verdict

